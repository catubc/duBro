{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "%matplotlib tk\n",
    "#%autosave 180\n",
    "import matplotlib.pyplot as plt\n",
    "#from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128, 160)\n",
      "(10000, 128, 160)\n"
     ]
    }
   ],
   "source": [
    "# LOAD the first 10,000 PREDICTIONS/CONFIDENCE MAPS THAT COME OUT OF DEEPLABCUT for a particular video\n",
    "# Notes: \n",
    "#        Equivalent to 10000/25fps = 400seconds\n",
    "#        Dimensionts: [# of Frames, width, height]\n",
    "#        DLC PREDICTS ON 1280 x 1024 images, but they are downsampled to 128 x 160\n",
    "#               i.e. if we need higher resolution, it can be obtained easily\n",
    "\n",
    "predictions = np.load('/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/predictions_10k_new_surfaces.npy')\n",
    "print (predictions.shape)\n",
    "\n",
    "max_frames = 10000\n",
    "\n",
    "predictions=predictions[:max_frames]\n",
    "print (predictions.shape)\n",
    "\n",
    "# im = plt.imshow(predictions[0])\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# REPROCESS HEATMAPS\n",
    "for k in range(predictions.shape[0]):\n",
    "    predictions[k]=(predictions[k]-np.min(predictions[k]))/(np.max(predictions[k]-np.min(predictions[k])))\n",
    "\n",
    "threshold = .7\n",
    "idx = np.where(predictions<threshold)\n",
    "predictions[idx]=0\n",
    "\n",
    "for k in range(predictions.shape[0]):\n",
    "    predictions[k]=(predictions[k]-np.min(predictions[k]))/(np.max(predictions[k]-np.min(predictions[k])))\n",
    "\n",
    "print (np.max(predictions[0]), np.min(predictions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.01568627450980392\n"
     ]
    }
   ],
   "source": [
    "print (np.max(images[100]), np.min(images[100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('/home/cat/prediction0.npy', predictions[0])\n",
    "# np.save('/home/cat/images0.npy',images[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n"
     ]
    }
   ],
   "source": [
    "# LOAD THE FIRST 1,000 VIDEO FRAMES (there are 10k in the saved file)\n",
    "\n",
    "import cv2\n",
    "vidcap = cv2.VideoCapture('/media/cat/4TBSSD/dan/march_2/madeline_dlc/march_16/2020-3-16_12_54_07_193951_compressed/2020-3-16_12_54_07_193951_compressed.avi')\n",
    "images = np.zeros((predictions.shape[0],\n",
    "                  predictions.shape[1],\n",
    "                  predictions.shape[2],\n",
    "                  3))\n",
    "count=0\n",
    "max_count = max_frames # THERE ARE 10,000 frames here;\n",
    "while True:\n",
    "    success, image = vidcap.read()\n",
    "    images[count]=image[::8,::8]\n",
    "    if count%100==0:\n",
    "        print (count)\n",
    "    count += 1\n",
    "    if count>=max_count:\n",
    "        break\n",
    "\n",
    "images = np.transpose(np.array(images),(0,-1,1,2))\n",
    "images = images/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # VISUALIZE AS SANITY CHECK SIDE BY SIDE\n",
    "# # SELECT FRAME\n",
    "\n",
    "# frame = 50\n",
    "\n",
    "# ax=plt.subplot(2,2,1)\n",
    "# plt.title(\"Raw video frame\",fontsize=16)\n",
    "# #print (images[frame].shape)\n",
    "# plt.imshow(images[frame].sum(0),alpha=1)\n",
    "# ax=plt.subplot(2,2,2)\n",
    "# plt.title(\"DLC (sum) heatmaps\",fontsize=16)\n",
    "# plt.imshow(predictions[frame].squeeze(),alpha=1)\n",
    "# ax=plt.subplot(2,2,3)\n",
    "# plt.title(\"Overlayed as a sanity check\",fontsize=16)\n",
    "# plt.imshow(images[frame].sum(0),alpha=.5)\n",
    "# plt.imshow(predictions[frame].squeeze(),alpha=.5)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cnndnn(nn.Module):\n",
    "    def __init__(self,hdim=48):\n",
    "        super(Cnndnn, self).__init__()\n",
    "        \n",
    "        #input channels, output channels, kernel, stride\n",
    "        self.conv1 = nn.Conv2d(3,32,3,1)\n",
    "        self.conv2 = nn.Conv2d(32,64,3,1)\n",
    "        self.conv3 = nn.Conv2d(64,128,3,1)\n",
    "        self.conv4 = nn.Conv2d(128,256,3,1)\n",
    "        \n",
    "        self.deconv4 = nn.ConvTranspose2d(256,128,3,1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(128,64,3,1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(64,32,3,1)\n",
    "        self.deconv1 = nn.ConvTranspose2d(32,1,3,1)\n",
    "        \n",
    "        self.linearconv = nn.Linear(12288,hdim)\n",
    "        self.lineardeconv = nn.Linear(hdim,12288)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, return_indices=True)\n",
    "        self.unpool = nn.MaxUnpool2d(2)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        #pass through generic CNN\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x,idx1 = self.pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x,idx2 = self.pool(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x,idx3 = self.pool(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x,idx4 = self.pool(x)\n",
    "        \n",
    "        #transform cnn output to the 'hidden'/encoding layer thing\n",
    "        x = self.linearconv(torch.flatten(x,1))\n",
    "        \n",
    "        #do rnn things here if desired:\n",
    "        # x = rnn(x,xprev)\n",
    "        \n",
    "        #transform back to 'feature'-space\n",
    "        x = self.lineardeconv(x)\n",
    "        x = x.view(-1,256,6,8)\n",
    "\n",
    "        #blow back out with deconvNN\n",
    "        x = self.unpool(x,idx4,output_size=(12,16))\n",
    "        x = F.relu(x)\n",
    "        x = self.deconv4(x)\n",
    "\n",
    "        x = self.unpool(x,idx3,output_size=(28,36))\n",
    "        x = F.relu(x)\n",
    "        x = self.deconv3(x)\n",
    "        \n",
    "        x = self.unpool(x,idx2,output_size=(61,77))\n",
    "        x = F.relu(x)\n",
    "        x = self.deconv2(x)\n",
    "        \n",
    "        x = self.unpool(x,idx1,output_size=(126,158))\n",
    "        x = F.relu(x)\n",
    "        x = self.deconv1(x)\n",
    "        \n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "cnndnn = Cnndnn().to(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#foo = cnndnn(torch.rand(10,3,128,160))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_data_np = np.random.rand(100,3,128,160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heatmaps_np = np.random.rand(100,3,128,160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #predictions = predictions [:100]\n",
    "# print (predictions.shape)\n",
    "# print (images.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = torch.utils.data.TensorDataset(torch.tensor(img_data_np).float(),torch.tensor(heatmaps_np).float())\n",
    "dataset = torch.utils.data.TensorDataset(torch.tensor(images).float(),torch.tensor(predictions).float())\n",
    "data_loader = torch.utils.data.DataLoader(dataset,batch_size=10,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOCHASTIC GD\n",
    "# optimizer = optim.SGD(cnndnn.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# SECOND OPTION\n",
    "optimizer = optim.Adam(cnndnn.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 0.016355004161596298\n",
      "iter 1, loss: 0.014792589470744133\n",
      "iter 2, loss: 0.014591893181204796\n",
      "iter 3, loss: 0.012872978113591671\n",
      "iter 4, loss: 0.01579000987112522\n",
      "iter 5, loss: 0.013936401344835758\n",
      "iter 6, loss: 0.015597126446664333\n",
      "iter 7, loss: 0.014099931344389915\n",
      "iter 8, loss: 0.01513295341283083\n",
      "iter 9, loss: 0.016701053828001022\n",
      "iter 10, loss: 0.016882119700312614\n",
      "iter 11, loss: 0.017583508044481277\n",
      "iter 12, loss: 0.01598624885082245\n",
      "iter 13, loss: 0.014567789621651173\n",
      "iter 14, loss: 0.015388421714305878\n",
      "iter 15, loss: 0.018162574619054794\n",
      "iter 16, loss: 0.01653827354311943\n",
      "iter 17, loss: 0.017816444858908653\n",
      "iter 18, loss: 0.019425617530941963\n",
      "iter 19, loss: 0.015474935062229633\n",
      "iter 20, loss: 0.01564696989953518\n",
      "iter 21, loss: 0.01646873913705349\n",
      "iter 22, loss: 0.013868691399693489\n",
      "iter 23, loss: 0.015776963904500008\n",
      "iter 24, loss: 0.014495806768536568\n",
      "iter 25, loss: 0.014976256527006626\n",
      "iter 26, loss: 0.015755033120512962\n",
      "iter 27, loss: 0.015502966940402985\n",
      "iter 28, loss: 0.015457446686923504\n",
      "iter 29, loss: 0.015015671029686928\n",
      "iter 30, loss: 0.013711744919419289\n",
      "iter 31, loss: 0.015116827562451363\n",
      "iter 32, loss: 0.014397854916751385\n",
      "iter 33, loss: 0.015944821760058403\n",
      "iter 34, loss: 0.013955054804682732\n",
      "iter 35, loss: 0.014927259646356106\n",
      "iter 36, loss: 0.014564302749931812\n",
      "iter 37, loss: 0.013560031540691853\n",
      "iter 38, loss: 0.015315372496843338\n",
      "iter 39, loss: 0.015527969226241112\n",
      "iter 40, loss: 0.014907960779964924\n",
      "iter 41, loss: 0.01553996279835701\n",
      "iter 42, loss: 0.014898535795509815\n",
      "iter 43, loss: 0.014723881147801876\n",
      "iter 44, loss: 0.01418350636959076\n",
      "iter 45, loss: 0.015137293376028538\n",
      "iter 46, loss: 0.015192373655736446\n",
      "iter 47, loss: 0.014764856547117233\n",
      "iter 48, loss: 0.01429516077041626\n",
      "iter 49, loss: 0.013483265414834023\n",
      "iter 50, loss: 0.014989277347922325\n",
      "iter 51, loss: 0.013600721955299377\n",
      "iter 52, loss: 0.012341625057160854\n",
      "iter 53, loss: 0.014046823605895042\n",
      "iter 54, loss: 0.015297184698283672\n",
      "iter 55, loss: 0.01389161217957735\n",
      "iter 56, loss: 0.01283752080053091\n",
      "iter 57, loss: 0.01275035459548235\n",
      "iter 58, loss: 0.012626055628061295\n",
      "iter 59, loss: 0.010195035487413406\n",
      "iter 60, loss: 0.013400883413851261\n",
      "iter 61, loss: 0.013021481223404408\n",
      "iter 62, loss: 0.012785594910383224\n",
      "iter 63, loss: 0.013343838043510914\n",
      "iter 64, loss: 0.012407470494508743\n",
      "iter 65, loss: 0.011759358458220959\n",
      "iter 66, loss: 0.012678740546107292\n",
      "iter 67, loss: 0.013135978020727634\n",
      "iter 68, loss: 0.013097116723656654\n",
      "iter 69, loss: 0.013248786330223083\n",
      "iter 70, loss: 0.012166760861873627\n",
      "iter 71, loss: 0.013409190811216831\n",
      "iter 72, loss: 0.013304474763572216\n",
      "iter 73, loss: 0.011993944644927979\n",
      "iter 74, loss: 0.01295960322022438\n",
      "iter 75, loss: 0.01073899120092392\n",
      "iter 76, loss: 0.011263958178460598\n",
      "iter 77, loss: 0.012670161202549934\n",
      "iter 78, loss: 0.013025190681219101\n",
      "iter 79, loss: 0.010897773317992687\n",
      "iter 80, loss: 0.011680511757731438\n",
      "iter 81, loss: 0.010836402885615826\n",
      "iter 82, loss: 0.012959284707903862\n",
      "iter 83, loss: 0.01324139442294836\n",
      "iter 84, loss: 0.012916112318634987\n",
      "iter 85, loss: 0.011699947528541088\n",
      "iter 86, loss: 0.011023575440049171\n",
      "iter 87, loss: 0.010402879677712917\n",
      "iter 88, loss: 0.01199360378086567\n",
      "iter 89, loss: 0.011899339966475964\n",
      "iter 90, loss: 0.013131765648722649\n",
      "iter 91, loss: 0.012163341045379639\n",
      "iter 92, loss: 0.012348203919827938\n",
      "iter 93, loss: 0.011727101169526577\n",
      "iter 94, loss: 0.011629816144704819\n",
      "iter 95, loss: 0.01223700400441885\n",
      "iter 96, loss: 0.011593270115554333\n",
      "iter 97, loss: 0.012415268458425999\n",
      "iter 98, loss: 0.01171447429805994\n",
      "iter 99, loss: 0.009652381762862206\n",
      "iter 100, loss: 0.011282823048532009\n",
      "iter 101, loss: 0.01167789101600647\n",
      "iter 102, loss: 0.010580570437014103\n",
      "iter 103, loss: 0.01409829780459404\n",
      "iter 104, loss: 0.011820653453469276\n",
      "iter 105, loss: 0.011728215962648392\n",
      "iter 106, loss: 0.012585104443132877\n",
      "iter 107, loss: 0.010570797137916088\n",
      "iter 108, loss: 0.010643263347446918\n",
      "iter 109, loss: 0.009836125187575817\n",
      "iter 110, loss: 0.009997150860726833\n",
      "iter 111, loss: 0.012394317425787449\n",
      "iter 112, loss: 0.010827361606061459\n",
      "iter 113, loss: 0.011560482904314995\n",
      "iter 114, loss: 0.011629398912191391\n",
      "iter 115, loss: 0.011354552581906319\n",
      "iter 116, loss: 0.011159617453813553\n",
      "iter 117, loss: 0.011989645659923553\n",
      "iter 118, loss: 0.01079519558697939\n",
      "iter 119, loss: 0.010949567891657352\n",
      "iter 120, loss: 0.010763105936348438\n",
      "iter 121, loss: 0.010466888546943665\n",
      "iter 122, loss: 0.012630529701709747\n",
      "iter 123, loss: 0.011880016885697842\n",
      "iter 124, loss: 0.011727385222911835\n",
      "iter 125, loss: 0.010640211403369904\n",
      "iter 126, loss: 0.01108643226325512\n",
      "iter 127, loss: 0.01109224185347557\n",
      "iter 128, loss: 0.011966358870267868\n",
      "iter 129, loss: 0.011804964393377304\n",
      "iter 130, loss: 0.013030661270022392\n",
      "iter 131, loss: 0.01180539932101965\n",
      "iter 132, loss: 0.011517450213432312\n",
      "iter 133, loss: 0.009498718194663525\n",
      "iter 134, loss: 0.012515597976744175\n",
      "iter 135, loss: 0.011067880317568779\n",
      "iter 136, loss: 0.010387035086750984\n",
      "iter 137, loss: 0.011742446571588516\n",
      "iter 138, loss: 0.01064450852572918\n",
      "iter 139, loss: 0.01147778145968914\n",
      "iter 140, loss: 0.00913195963948965\n",
      "iter 141, loss: 0.011844897642731667\n",
      "iter 142, loss: 0.010761179029941559\n",
      "iter 143, loss: 0.009817787446081638\n",
      "iter 144, loss: 0.011215804144740105\n",
      "iter 145, loss: 0.01115418877452612\n",
      "iter 146, loss: 0.011860419064760208\n",
      "iter 147, loss: 0.011421170085668564\n",
      "iter 148, loss: 0.01000809296965599\n",
      "iter 149, loss: 0.011358859948813915\n",
      "iter 150, loss: 0.009717654436826706\n",
      "iter 151, loss: 0.011196485720574856\n",
      "iter 152, loss: 0.010037141852080822\n",
      "iter 153, loss: 0.011823723092675209\n",
      "iter 154, loss: 0.011480757035315037\n",
      "iter 155, loss: 0.010441062971949577\n",
      "iter 156, loss: 0.010824726894497871\n",
      "iter 157, loss: 0.011851463466882706\n",
      "iter 158, loss: 0.011238369159400463\n",
      "iter 159, loss: 0.014003170654177666\n",
      "iter 160, loss: 0.012504023499786854\n",
      "iter 161, loss: 0.011606807820498943\n",
      "iter 162, loss: 0.011067402549088001\n",
      "iter 163, loss: 0.011127050966024399\n",
      "iter 164, loss: 0.010405093431472778\n",
      "iter 165, loss: 0.011238590814173222\n",
      "iter 166, loss: 0.010074427351355553\n",
      "iter 167, loss: 0.011485612951219082\n",
      "iter 168, loss: 0.01093453448265791\n",
      "iter 169, loss: 0.012416454963386059\n",
      "iter 170, loss: 0.01051334198564291\n",
      "iter 171, loss: 0.010525071062147617\n",
      "iter 172, loss: 0.010848204605281353\n",
      "iter 173, loss: 0.008800705894827843\n",
      "iter 174, loss: 0.010098789818584919\n",
      "iter 175, loss: 0.011336471885442734\n",
      "iter 176, loss: 0.011571025475859642\n",
      "iter 177, loss: 0.010945864021778107\n",
      "iter 178, loss: 0.010820609517395496\n",
      "iter 179, loss: 0.010584206320345402\n",
      "iter 180, loss: 0.010618416592478752\n",
      "iter 181, loss: 0.010150439105927944\n",
      "iter 182, loss: 0.010941820219159126\n",
      "iter 183, loss: 0.010692468844354153\n",
      "iter 184, loss: 0.009996620938181877\n",
      "iter 185, loss: 0.010664615780115128\n",
      "iter 186, loss: 0.01202027965337038\n",
      "iter 187, loss: 0.009606792591512203\n",
      "iter 188, loss: 0.010401283390820026\n",
      "iter 189, loss: 0.011499439366161823\n",
      "iter 190, loss: 0.009210480377078056\n",
      "iter 191, loss: 0.011235078796744347\n",
      "iter 192, loss: 0.012799660675227642\n",
      "iter 193, loss: 0.01168894674628973\n",
      "iter 194, loss: 0.010715574957430363\n",
      "iter 195, loss: 0.010380520485341549\n",
      "iter 196, loss: 0.011910418048501015\n",
      "iter 197, loss: 0.010225060395896435\n",
      "iter 198, loss: 0.010667354799807072\n",
      "iter 199, loss: 0.01158241555094719\n",
      "iter 200, loss: 0.012023289687931538\n",
      "iter 201, loss: 0.01112621370702982\n",
      "iter 202, loss: 0.011182189919054508\n",
      "iter 203, loss: 0.011280709877610207\n",
      "iter 204, loss: 0.009804695844650269\n",
      "iter 205, loss: 0.009678690694272518\n",
      "iter 206, loss: 0.009978490881621838\n",
      "iter 207, loss: 0.012288759462535381\n",
      "iter 208, loss: 0.010338063351809978\n",
      "iter 209, loss: 0.011895114555954933\n",
      "iter 210, loss: 0.01093923207372427\n",
      "iter 211, loss: 0.010449588298797607\n",
      "iter 212, loss: 0.010014568455517292\n",
      "iter 213, loss: 0.012485033832490444\n",
      "iter 214, loss: 0.010412090457975864\n",
      "iter 215, loss: 0.009869602508842945\n",
      "iter 216, loss: 0.010488265193998814\n",
      "iter 217, loss: 0.009718170389533043\n",
      "iter 218, loss: 0.011752007529139519\n",
      "iter 219, loss: 0.011109833605587482\n",
      "iter 220, loss: 0.010149038396775723\n",
      "iter 221, loss: 0.011670355685055256\n",
      "iter 222, loss: 0.00949675403535366\n",
      "iter 223, loss: 0.01232415996491909\n",
      "iter 224, loss: 0.00988726131618023\n",
      "iter 225, loss: 0.011906586587429047\n",
      "iter 226, loss: 0.01077254768460989\n",
      "iter 227, loss: 0.009918836876749992\n",
      "iter 228, loss: 0.009004061110317707\n",
      "iter 229, loss: 0.012910740450024605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 230, loss: 0.010235314257442951\n",
      "iter 231, loss: 0.011037880554795265\n",
      "iter 232, loss: 0.011578169651329517\n",
      "iter 233, loss: 0.010492886416614056\n",
      "iter 234, loss: 0.011315676383674145\n",
      "iter 235, loss: 0.009558012709021568\n",
      "iter 236, loss: 0.010106232948601246\n",
      "iter 237, loss: 0.008276681415736675\n",
      "iter 238, loss: 0.010426374152302742\n",
      "iter 239, loss: 0.010992367751896381\n",
      "iter 240, loss: 0.011690828949213028\n",
      "iter 241, loss: 0.008822466246783733\n",
      "iter 242, loss: 0.011453337036073208\n",
      "iter 243, loss: 0.012088739313185215\n",
      "iter 244, loss: 0.010057464241981506\n",
      "iter 245, loss: 0.009533955715596676\n",
      "iter 246, loss: 0.011645735241472721\n",
      "iter 247, loss: 0.010203249752521515\n",
      "iter 248, loss: 0.009877406060695648\n",
      "iter 249, loss: 0.009769875556230545\n",
      "iter 250, loss: 0.01088436134159565\n",
      "iter 251, loss: 0.010709303431212902\n",
      "iter 252, loss: 0.011789735406637192\n",
      "iter 253, loss: 0.009813912212848663\n",
      "iter 254, loss: 0.010265651158988476\n",
      "iter 255, loss: 0.009775716811418533\n",
      "iter 256, loss: 0.013012335635721684\n",
      "iter 257, loss: 0.010950795374810696\n",
      "iter 258, loss: 0.009182794950902462\n",
      "iter 259, loss: 0.01202904898673296\n",
      "iter 260, loss: 0.009755762293934822\n",
      "iter 261, loss: 0.009379936382174492\n",
      "iter 262, loss: 0.01145548652857542\n",
      "iter 263, loss: 0.010175703093409538\n",
      "iter 264, loss: 0.011774053797125816\n",
      "iter 265, loss: 0.010049344040453434\n",
      "iter 266, loss: 0.011443102732300758\n",
      "iter 267, loss: 0.009753446094691753\n",
      "iter 268, loss: 0.010816087014973164\n",
      "iter 269, loss: 0.011603543534874916\n",
      "iter 270, loss: 0.012049002572894096\n",
      "iter 271, loss: 0.013240091502666473\n",
      "iter 272, loss: 0.011281280778348446\n",
      "iter 273, loss: 0.010589482262730598\n",
      "iter 274, loss: 0.013070633634924889\n",
      "iter 275, loss: 0.010434292256832123\n",
      "iter 276, loss: 0.01101809274405241\n",
      "iter 277, loss: 0.011079331859946251\n",
      "iter 278, loss: 0.010345138609409332\n",
      "iter 279, loss: 0.01016677264124155\n",
      "iter 280, loss: 0.010286868549883366\n",
      "iter 281, loss: 0.010219741612672806\n",
      "iter 282, loss: 0.009976972825825214\n",
      "iter 283, loss: 0.010757301934063435\n",
      "iter 284, loss: 0.011981712654232979\n",
      "iter 285, loss: 0.010173557326197624\n",
      "iter 286, loss: 0.010517781600356102\n",
      "iter 287, loss: 0.011577270925045013\n",
      "iter 288, loss: 0.01098328921943903\n",
      "iter 289, loss: 0.010586559772491455\n",
      "iter 290, loss: 0.01065039075911045\n",
      "iter 291, loss: 0.01115320436656475\n",
      "iter 292, loss: 0.009969108738005161\n",
      "iter 293, loss: 0.009963804855942726\n",
      "iter 294, loss: 0.01072826236486435\n",
      "iter 295, loss: 0.009922824800014496\n",
      "iter 296, loss: 0.010332074016332626\n",
      "iter 297, loss: 0.01109679788351059\n",
      "iter 298, loss: 0.010473795235157013\n",
      "iter 299, loss: 0.011935139074921608\n",
      "iter 300, loss: 0.011688664555549622\n",
      "iter 301, loss: 0.01169925183057785\n",
      "iter 302, loss: 0.012974271550774574\n",
      "iter 303, loss: 0.011050078086555004\n",
      "iter 304, loss: 0.010638192296028137\n",
      "iter 305, loss: 0.011087688617408276\n",
      "iter 306, loss: 0.010909518226981163\n",
      "iter 307, loss: 0.011159427464008331\n",
      "iter 308, loss: 0.009814953431487083\n",
      "iter 309, loss: 0.009678876027464867\n",
      "iter 310, loss: 0.011267496272921562\n",
      "iter 311, loss: 0.00980593916028738\n",
      "iter 312, loss: 0.01137362141162157\n",
      "iter 313, loss: 0.01119981613010168\n",
      "iter 314, loss: 0.011155099608004093\n",
      "iter 315, loss: 0.010629013180732727\n",
      "iter 316, loss: 0.010923861525952816\n",
      "iter 317, loss: 0.011320550926029682\n",
      "iter 318, loss: 0.011066464707255363\n",
      "iter 319, loss: 0.0112377954646945\n",
      "iter 320, loss: 0.012088173069059849\n",
      "iter 321, loss: 0.008963953703641891\n",
      "iter 322, loss: 0.012325373478233814\n",
      "iter 323, loss: 0.010741776786744595\n",
      "iter 324, loss: 0.010853160172700882\n",
      "iter 325, loss: 0.010745827108621597\n",
      "iter 326, loss: 0.010254313237965107\n",
      "iter 327, loss: 0.010340908542275429\n",
      "iter 328, loss: 0.011857599951326847\n",
      "iter 329, loss: 0.01144449133425951\n",
      "iter 330, loss: 0.010246929712593555\n",
      "iter 331, loss: 0.0115741565823555\n",
      "iter 332, loss: 0.009535932913422585\n",
      "iter 333, loss: 0.009306082502007484\n",
      "iter 334, loss: 0.010679171420633793\n",
      "iter 335, loss: 0.010160258039832115\n",
      "iter 336, loss: 0.009548119269311428\n",
      "iter 337, loss: 0.011996434070169926\n",
      "iter 338, loss: 0.011085041798651218\n",
      "iter 339, loss: 0.01211376953870058\n",
      "iter 340, loss: 0.010001343674957752\n",
      "iter 341, loss: 0.009428093209862709\n",
      "iter 342, loss: 0.009975030086934566\n",
      "iter 343, loss: 0.010181830264627934\n",
      "iter 344, loss: 0.00900947954505682\n",
      "iter 345, loss: 0.010057640261948109\n",
      "iter 346, loss: 0.008741184137761593\n",
      "iter 347, loss: 0.01041506789624691\n",
      "iter 348, loss: 0.009244010783731937\n",
      "iter 349, loss: 0.010892062447965145\n",
      "iter 350, loss: 0.01074584573507309\n",
      "iter 351, loss: 0.011345400474965572\n",
      "iter 352, loss: 0.009458399377763271\n",
      "iter 353, loss: 0.011091932654380798\n",
      "iter 354, loss: 0.009881310164928436\n",
      "iter 355, loss: 0.01000046543776989\n",
      "iter 356, loss: 0.00883824285119772\n",
      "iter 357, loss: 0.009564528241753578\n",
      "iter 358, loss: 0.009347667917609215\n",
      "iter 359, loss: 0.01028802152723074\n",
      "iter 360, loss: 0.009687249548733234\n",
      "iter 361, loss: 0.010744037106633186\n",
      "iter 362, loss: 0.010161352343857288\n",
      "iter 363, loss: 0.00965791754424572\n",
      "iter 364, loss: 0.011332094669342041\n",
      "iter 365, loss: 0.01156129315495491\n",
      "iter 366, loss: 0.011245223693549633\n",
      "iter 367, loss: 0.010259809903800488\n",
      "iter 368, loss: 0.010959191247820854\n",
      "iter 369, loss: 0.011804929934442043\n",
      "iter 370, loss: 0.010484867729246616\n",
      "iter 371, loss: 0.010839143767952919\n",
      "iter 372, loss: 0.011896057985723019\n",
      "iter 373, loss: 0.009860090911388397\n",
      "iter 374, loss: 0.010472300462424755\n",
      "iter 375, loss: 0.01084893848747015\n",
      "iter 376, loss: 0.01118898019194603\n",
      "iter 377, loss: 0.01179097592830658\n",
      "iter 378, loss: 0.00905587524175644\n",
      "iter 379, loss: 0.010081196203827858\n",
      "iter 380, loss: 0.011090653948485851\n",
      "iter 381, loss: 0.009677124209702015\n",
      "iter 382, loss: 0.010295003652572632\n",
      "iter 383, loss: 0.010504252277314663\n",
      "iter 384, loss: 0.010156223550438881\n",
      "iter 385, loss: 0.01057368516921997\n",
      "iter 386, loss: 0.009638997726142406\n",
      "iter 387, loss: 0.011514276266098022\n",
      "iter 388, loss: 0.011375595815479755\n",
      "iter 389, loss: 0.011378244496881962\n",
      "iter 390, loss: 0.010015017352998257\n",
      "iter 391, loss: 0.010359959676861763\n",
      "iter 392, loss: 0.01222720742225647\n",
      "iter 393, loss: 0.01062889862805605\n",
      "iter 394, loss: 0.011320521123707294\n",
      "iter 395, loss: 0.010058682411909103\n",
      "iter 396, loss: 0.011397436261177063\n",
      "iter 397, loss: 0.008749551139771938\n",
      "iter 398, loss: 0.010430941358208656\n",
      "iter 399, loss: 0.010366060771048069\n",
      "iter 400, loss: 0.010626601055264473\n",
      "iter 401, loss: 0.01084886770695448\n",
      "iter 402, loss: 0.010051565244793892\n",
      "iter 403, loss: 0.009786238893866539\n",
      "iter 404, loss: 0.01134735718369484\n",
      "iter 405, loss: 0.010795202106237411\n",
      "iter 406, loss: 0.010091970674693584\n",
      "iter 407, loss: 0.010248490609228611\n",
      "iter 408, loss: 0.012594912201166153\n",
      "iter 409, loss: 0.010039249435067177\n",
      "iter 410, loss: 0.010281899012625217\n",
      "iter 411, loss: 0.010212589055299759\n",
      "iter 412, loss: 0.010342353954911232\n",
      "iter 413, loss: 0.00955527275800705\n",
      "iter 414, loss: 0.010838648304343224\n",
      "iter 415, loss: 0.011859692633152008\n",
      "iter 416, loss: 0.012661302462220192\n",
      "iter 417, loss: 0.011943511664867401\n",
      "iter 418, loss: 0.011218330822885036\n",
      "iter 419, loss: 0.011043194681406021\n",
      "iter 420, loss: 0.01115073636174202\n",
      "iter 421, loss: 0.011181139387190342\n",
      "iter 422, loss: 0.009665945544838905\n",
      "iter 423, loss: 0.010357553139328957\n",
      "iter 424, loss: 0.01054108515381813\n",
      "iter 425, loss: 0.011111180298030376\n",
      "iter 426, loss: 0.009892093017697334\n",
      "iter 427, loss: 0.011351964436471462\n",
      "iter 428, loss: 0.012076213955879211\n",
      "iter 429, loss: 0.010397759266197681\n",
      "iter 430, loss: 0.011050846427679062\n",
      "iter 431, loss: 0.009601102210581303\n",
      "iter 432, loss: 0.009033323265612125\n",
      "iter 433, loss: 0.009848766028881073\n",
      "iter 434, loss: 0.011736023239791393\n",
      "iter 435, loss: 0.011099199764430523\n",
      "iter 436, loss: 0.012328028678894043\n",
      "iter 437, loss: 0.011295479722321033\n",
      "iter 438, loss: 0.009393696673214436\n",
      "iter 439, loss: 0.010604367591440678\n",
      "iter 440, loss: 0.010731731541454792\n",
      "iter 441, loss: 0.011830855160951614\n",
      "iter 442, loss: 0.010479234158992767\n",
      "iter 443, loss: 0.01167852059006691\n",
      "iter 444, loss: 0.009655398316681385\n",
      "iter 445, loss: 0.011282799765467644\n",
      "iter 446, loss: 0.008853644132614136\n",
      "iter 447, loss: 0.009417352266609669\n",
      "iter 448, loss: 0.010628663003444672\n",
      "iter 449, loss: 0.009548508562147617\n",
      "iter 450, loss: 0.00932880025357008\n",
      "iter 451, loss: 0.008369718678295612\n",
      "iter 452, loss: 0.010831546038389206\n",
      "iter 453, loss: 0.010561379604041576\n",
      "iter 454, loss: 0.010862438008189201\n",
      "iter 455, loss: 0.010037651285529137\n",
      "iter 456, loss: 0.012797709554433823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 457, loss: 0.012454205192625523\n",
      "iter 458, loss: 0.007970437407493591\n",
      "iter 459, loss: 0.011106523685157299\n",
      "iter 460, loss: 0.01256287470459938\n",
      "iter 461, loss: 0.011322543025016785\n",
      "iter 462, loss: 0.01086342241615057\n",
      "iter 463, loss: 0.012018950656056404\n",
      "iter 464, loss: 0.01095682941377163\n",
      "iter 465, loss: 0.01242367085069418\n",
      "iter 466, loss: 0.011477169580757618\n",
      "iter 467, loss: 0.010068594478070736\n",
      "iter 468, loss: 0.010713291354477406\n",
      "iter 469, loss: 0.010602173395454884\n",
      "iter 470, loss: 0.009767798706889153\n",
      "iter 471, loss: 0.010943601839244366\n",
      "iter 472, loss: 0.009700260125100613\n",
      "iter 473, loss: 0.01156742125749588\n",
      "iter 474, loss: 0.011418755166232586\n",
      "iter 475, loss: 0.011122776195406914\n",
      "iter 476, loss: 0.009572319686412811\n",
      "iter 477, loss: 0.010822320356965065\n",
      "iter 478, loss: 0.011174860410392284\n",
      "iter 479, loss: 0.011113710701465607\n",
      "iter 480, loss: 0.008369590155780315\n",
      "iter 481, loss: 0.010362706147134304\n",
      "iter 482, loss: 0.009596926160156727\n",
      "iter 483, loss: 0.011628607288002968\n",
      "iter 484, loss: 0.010414374060928822\n",
      "iter 485, loss: 0.009613202884793282\n",
      "iter 486, loss: 0.011188528500497341\n",
      "iter 487, loss: 0.010370583273470402\n",
      "iter 488, loss: 0.009795957244932652\n",
      "iter 489, loss: 0.010957027785480022\n",
      "iter 490, loss: 0.01036336924880743\n",
      "iter 491, loss: 0.011978487484157085\n",
      "iter 492, loss: 0.011364801786839962\n",
      "iter 493, loss: 0.010787229984998703\n",
      "iter 494, loss: 0.010774130001664162\n",
      "iter 495, loss: 0.011696798726916313\n",
      "iter 496, loss: 0.010983963496983051\n",
      "iter 497, loss: 0.012885289266705513\n",
      "iter 498, loss: 0.009546001441776752\n",
      "iter 499, loss: 0.009328914806246758\n",
      "iter 500, loss: 0.011824090033769608\n",
      "iter 501, loss: 0.011368277482688427\n",
      "iter 502, loss: 0.011333933100104332\n",
      "iter 503, loss: 0.01048794761300087\n",
      "iter 504, loss: 0.010338833555579185\n",
      "iter 505, loss: 0.00928967073559761\n",
      "iter 506, loss: 0.011290728114545345\n",
      "iter 507, loss: 0.00939187966287136\n",
      "iter 508, loss: 0.01061011478304863\n",
      "iter 509, loss: 0.011071572080254555\n",
      "iter 510, loss: 0.01015845499932766\n",
      "iter 511, loss: 0.009460494853556156\n",
      "iter 512, loss: 0.010648228228092194\n",
      "iter 513, loss: 0.011013278737664223\n",
      "iter 514, loss: 0.010999022051692009\n",
      "iter 515, loss: 0.009639381431043148\n",
      "iter 516, loss: 0.009679460898041725\n",
      "iter 517, loss: 0.011204427108168602\n",
      "iter 518, loss: 0.011691350489854813\n",
      "iter 519, loss: 0.010448838584125042\n",
      "iter 520, loss: 0.011280626058578491\n",
      "iter 521, loss: 0.01064328383654356\n",
      "iter 522, loss: 0.011503040790557861\n",
      "iter 523, loss: 0.009321647696197033\n",
      "iter 524, loss: 0.010692746378481388\n",
      "iter 525, loss: 0.010164754465222359\n",
      "iter 526, loss: 0.011298619210720062\n",
      "iter 527, loss: 0.009763531386852264\n",
      "iter 528, loss: 0.01065270509570837\n",
      "iter 529, loss: 0.010597359389066696\n",
      "iter 530, loss: 0.011706321500241756\n",
      "iter 531, loss: 0.011565768159925938\n",
      "iter 532, loss: 0.009693996049463749\n",
      "iter 533, loss: 0.011228460818529129\n",
      "iter 534, loss: 0.010049896314740181\n",
      "iter 535, loss: 0.010210899636149406\n",
      "iter 536, loss: 0.01011014636605978\n",
      "iter 537, loss: 0.009097476489841938\n",
      "iter 538, loss: 0.009603410959243774\n",
      "iter 539, loss: 0.01012565940618515\n",
      "iter 540, loss: 0.009665489196777344\n",
      "iter 541, loss: 0.009321816265583038\n",
      "iter 542, loss: 0.011668377555906773\n",
      "iter 543, loss: 0.009905476123094559\n",
      "iter 544, loss: 0.009879174642264843\n",
      "iter 545, loss: 0.00903287436813116\n",
      "iter 546, loss: 0.010128737427294254\n",
      "iter 547, loss: 0.011911948211491108\n",
      "iter 548, loss: 0.01029067300260067\n",
      "iter 549, loss: 0.010549192316830158\n",
      "iter 550, loss: 0.012425937689840794\n",
      "iter 551, loss: 0.009494411759078503\n",
      "iter 552, loss: 0.010730128735303879\n",
      "iter 553, loss: 0.010127924382686615\n",
      "iter 554, loss: 0.01183779165148735\n",
      "iter 555, loss: 0.010889251716434956\n",
      "iter 556, loss: 0.010418064892292023\n",
      "iter 557, loss: 0.00943424180150032\n",
      "iter 558, loss: 0.009309343993663788\n",
      "iter 559, loss: 0.011685820296406746\n",
      "iter 560, loss: 0.010813095606863499\n",
      "iter 561, loss: 0.01170713733881712\n",
      "iter 562, loss: 0.011586645618081093\n",
      "iter 563, loss: 0.01026379968971014\n",
      "iter 564, loss: 0.010056894272565842\n",
      "iter 565, loss: 0.009916113689541817\n",
      "iter 566, loss: 0.010495838709175587\n",
      "iter 567, loss: 0.010776812210679054\n",
      "iter 568, loss: 0.009978844784200191\n",
      "iter 569, loss: 0.009085945785045624\n",
      "iter 570, loss: 0.012876737862825394\n",
      "iter 571, loss: 0.013563301414251328\n",
      "iter 572, loss: 0.010496390052139759\n",
      "iter 573, loss: 0.010855450294911861\n",
      "iter 574, loss: 0.012144019827246666\n",
      "iter 575, loss: 0.010468857362866402\n",
      "iter 576, loss: 0.011429861187934875\n",
      "iter 577, loss: 0.011263483203947544\n",
      "iter 578, loss: 0.009407849051058292\n",
      "iter 579, loss: 0.009858105331659317\n",
      "iter 580, loss: 0.00943436287343502\n",
      "iter 581, loss: 0.01043675933033228\n",
      "iter 582, loss: 0.010648414492607117\n",
      "iter 583, loss: 0.010979481041431427\n",
      "iter 584, loss: 0.01095697470009327\n",
      "iter 585, loss: 0.009669958613812923\n",
      "iter 586, loss: 0.01035753171890974\n",
      "iter 587, loss: 0.011357476003468037\n",
      "iter 588, loss: 0.011748970486223698\n",
      "iter 589, loss: 0.010537254624068737\n",
      "iter 590, loss: 0.01033219788223505\n",
      "iter 591, loss: 0.009574093855917454\n",
      "iter 592, loss: 0.010752929374575615\n",
      "iter 593, loss: 0.010905707255005836\n",
      "iter 594, loss: 0.010129186324775219\n",
      "iter 595, loss: 0.010470841079950333\n",
      "iter 596, loss: 0.010330571793019772\n",
      "iter 597, loss: 0.010407873429358006\n",
      "iter 598, loss: 0.011373404413461685\n",
      "iter 599, loss: 0.011292913928627968\n",
      "iter 600, loss: 0.011459548026323318\n",
      "iter 601, loss: 0.009590351022779942\n",
      "iter 602, loss: 0.011374708265066147\n",
      "iter 603, loss: 0.01043881755322218\n",
      "iter 604, loss: 0.00954100675880909\n",
      "iter 605, loss: 0.010874500498175621\n",
      "iter 606, loss: 0.010331099852919579\n",
      "iter 607, loss: 0.00904292892664671\n",
      "iter 608, loss: 0.0097460662946105\n",
      "iter 609, loss: 0.010304797440767288\n",
      "iter 610, loss: 0.011089248582720757\n",
      "iter 611, loss: 0.011098810471594334\n",
      "iter 612, loss: 0.010456808842718601\n",
      "iter 613, loss: 0.008533699437975883\n",
      "iter 614, loss: 0.012027062475681305\n",
      "iter 615, loss: 0.009717369452118874\n",
      "iter 616, loss: 0.010175147093832493\n",
      "iter 617, loss: 0.010556463152170181\n",
      "iter 618, loss: 0.009077047929167747\n",
      "iter 619, loss: 0.012471312656998634\n",
      "iter 620, loss: 0.012210071086883545\n",
      "iter 621, loss: 0.01025837566703558\n",
      "iter 622, loss: 0.011338171549141407\n",
      "iter 623, loss: 0.010012542828917503\n",
      "iter 624, loss: 0.010409865528345108\n",
      "iter 625, loss: 0.010535042732954025\n",
      "iter 626, loss: 0.011887367814779282\n",
      "iter 627, loss: 0.011033117771148682\n",
      "iter 628, loss: 0.01123135071247816\n",
      "iter 629, loss: 0.011036055162549019\n",
      "iter 630, loss: 0.010686648078262806\n",
      "iter 631, loss: 0.009705746546387672\n",
      "iter 632, loss: 0.009725671261548996\n",
      "iter 633, loss: 0.01144338957965374\n",
      "iter 634, loss: 0.010062677785754204\n",
      "iter 635, loss: 0.011240418069064617\n",
      "iter 636, loss: 0.010185602121055126\n",
      "iter 637, loss: 0.009656498208642006\n",
      "iter 638, loss: 0.011297598481178284\n",
      "iter 639, loss: 0.011118915863335133\n",
      "iter 640, loss: 0.010181581601500511\n",
      "iter 641, loss: 0.012814752757549286\n",
      "iter 642, loss: 0.01140399370342493\n",
      "iter 643, loss: 0.012623515911400318\n",
      "iter 644, loss: 0.010752348229289055\n",
      "iter 645, loss: 0.010178987868130207\n",
      "iter 646, loss: 0.010869600810110569\n",
      "iter 647, loss: 0.010555732995271683\n",
      "iter 648, loss: 0.011319369077682495\n",
      "iter 649, loss: 0.009750168770551682\n",
      "iter 650, loss: 0.009743803180754185\n",
      "iter 651, loss: 0.01071422453969717\n",
      "iter 652, loss: 0.011081078089773655\n",
      "iter 653, loss: 0.010684657841920853\n",
      "iter 654, loss: 0.011342190206050873\n",
      "iter 655, loss: 0.009632385335862637\n",
      "iter 656, loss: 0.009906933642923832\n",
      "iter 657, loss: 0.009775090031325817\n",
      "iter 658, loss: 0.010679718106985092\n",
      "iter 659, loss: 0.011340462602674961\n",
      "iter 660, loss: 0.009615958668291569\n",
      "iter 661, loss: 0.010583964176476002\n",
      "iter 662, loss: 0.010847928002476692\n",
      "iter 663, loss: 0.009895636700093746\n",
      "iter 664, loss: 0.010896338149905205\n",
      "iter 665, loss: 0.010293841361999512\n",
      "iter 666, loss: 0.009893377311527729\n",
      "iter 667, loss: 0.010235949419438839\n",
      "iter 668, loss: 0.010073200799524784\n",
      "iter 669, loss: 0.01125132292509079\n",
      "iter 670, loss: 0.010603907518088818\n",
      "iter 671, loss: 0.011616365052759647\n",
      "iter 672, loss: 0.01016782596707344\n",
      "iter 673, loss: 0.009175382554531097\n",
      "iter 674, loss: 0.011604564264416695\n",
      "iter 675, loss: 0.011694835498929024\n",
      "iter 676, loss: 0.012059475295245647\n",
      "iter 677, loss: 0.011547226458787918\n",
      "iter 678, loss: 0.0103521179407835\n",
      "iter 679, loss: 0.010661802254617214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 680, loss: 0.010552250780165195\n",
      "iter 681, loss: 0.010261359624564648\n",
      "iter 682, loss: 0.011818783357739449\n",
      "iter 683, loss: 0.008494986221194267\n",
      "iter 684, loss: 0.010940895415842533\n",
      "iter 685, loss: 0.009685730561614037\n",
      "iter 686, loss: 0.01123658288270235\n",
      "iter 687, loss: 0.010519412346184254\n",
      "iter 688, loss: 0.010739484801888466\n",
      "iter 689, loss: 0.011116940528154373\n",
      "iter 690, loss: 0.01002185232937336\n",
      "iter 691, loss: 0.01056078914552927\n",
      "iter 692, loss: 0.010244389064610004\n",
      "iter 693, loss: 0.009769368916749954\n",
      "iter 694, loss: 0.010264530777931213\n",
      "iter 695, loss: 0.011528811417520046\n",
      "iter 696, loss: 0.009588856250047684\n",
      "iter 697, loss: 0.00944112055003643\n",
      "iter 698, loss: 0.011518394574522972\n",
      "iter 699, loss: 0.009207692928612232\n",
      "iter 700, loss: 0.011063316836953163\n",
      "iter 701, loss: 0.010829199105501175\n",
      "iter 702, loss: 0.009332781657576561\n",
      "iter 703, loss: 0.011237447150051594\n",
      "iter 704, loss: 0.010370845906436443\n",
      "iter 705, loss: 0.010074877180159092\n",
      "iter 706, loss: 0.009252602234482765\n",
      "iter 707, loss: 0.011073934845626354\n",
      "iter 708, loss: 0.010739350691437721\n",
      "iter 709, loss: 0.011242764070630074\n",
      "iter 710, loss: 0.008565387688577175\n",
      "iter 711, loss: 0.010259672999382019\n",
      "iter 712, loss: 0.01103703211992979\n",
      "iter 713, loss: 0.011595843359827995\n",
      "iter 714, loss: 0.012120299972593784\n",
      "iter 715, loss: 0.00993093941360712\n",
      "iter 716, loss: 0.010468529537320137\n",
      "iter 717, loss: 0.011411245912313461\n",
      "iter 718, loss: 0.011128776706755161\n",
      "iter 719, loss: 0.011315157637000084\n",
      "iter 720, loss: 0.011002276092767715\n",
      "iter 721, loss: 0.010493985377252102\n",
      "iter 722, loss: 0.009503371082246304\n",
      "iter 723, loss: 0.010757096111774445\n",
      "iter 724, loss: 0.010308835655450821\n",
      "iter 725, loss: 0.00875347014516592\n",
      "iter 726, loss: 0.011227654293179512\n",
      "iter 727, loss: 0.009784932248294353\n",
      "iter 728, loss: 0.010258987545967102\n",
      "iter 729, loss: 0.00924881361424923\n",
      "iter 730, loss: 0.011034870520234108\n",
      "iter 731, loss: 0.01005845982581377\n",
      "iter 732, loss: 0.011450775898993015\n",
      "iter 733, loss: 0.012380944564938545\n",
      "iter 734, loss: 0.011363809928297997\n",
      "iter 735, loss: 0.0079882200807333\n",
      "iter 736, loss: 0.010336156003177166\n",
      "iter 737, loss: 0.009330319240689278\n",
      "iter 738, loss: 0.010422619059681892\n",
      "iter 739, loss: 0.009407119825482368\n",
      "iter 740, loss: 0.011670376174151897\n",
      "iter 741, loss: 0.010776035487651825\n",
      "iter 742, loss: 0.010072021745145321\n",
      "iter 743, loss: 0.011057106778025627\n",
      "iter 744, loss: 0.009344656020402908\n",
      "iter 745, loss: 0.010572303086519241\n",
      "iter 746, loss: 0.012087013572454453\n",
      "iter 747, loss: 0.012100279331207275\n",
      "iter 748, loss: 0.010115775279700756\n",
      "iter 749, loss: 0.009649224579334259\n",
      "iter 750, loss: 0.01030427310615778\n",
      "iter 751, loss: 0.011168091557919979\n",
      "iter 752, loss: 0.012676387093961239\n",
      "iter 753, loss: 0.010528845712542534\n",
      "iter 754, loss: 0.010160744190216064\n",
      "iter 755, loss: 0.010182422585785389\n",
      "iter 756, loss: 0.010698110796511173\n",
      "iter 757, loss: 0.010807372629642487\n",
      "iter 758, loss: 0.008915279060602188\n",
      "iter 759, loss: 0.010705217719078064\n",
      "iter 760, loss: 0.008919729851186275\n",
      "iter 761, loss: 0.009041585959494114\n",
      "iter 762, loss: 0.010168629698455334\n",
      "iter 763, loss: 0.012515904381871223\n",
      "iter 764, loss: 0.009940494783222675\n",
      "iter 765, loss: 0.010390471667051315\n",
      "iter 766, loss: 0.009396308101713657\n",
      "iter 767, loss: 0.010215395130217075\n",
      "iter 768, loss: 0.01058710552752018\n",
      "iter 769, loss: 0.01035002525895834\n",
      "iter 770, loss: 0.009410553611814976\n",
      "iter 771, loss: 0.009323758073151112\n",
      "iter 772, loss: 0.009246161207556725\n",
      "iter 773, loss: 0.009856165386736393\n",
      "iter 774, loss: 0.009088781662285328\n",
      "iter 775, loss: 0.009673061780631542\n",
      "iter 776, loss: 0.012060747481882572\n",
      "iter 777, loss: 0.009242184460163116\n",
      "iter 778, loss: 0.01121138222515583\n",
      "iter 779, loss: 0.012173008173704147\n",
      "iter 780, loss: 0.010560083203017712\n",
      "iter 781, loss: 0.009536316618323326\n",
      "iter 782, loss: 0.010788703337311745\n",
      "iter 783, loss: 0.011256709694862366\n",
      "iter 784, loss: 0.011426608078181744\n",
      "iter 785, loss: 0.010458259843289852\n",
      "iter 786, loss: 0.012188808992505074\n",
      "iter 787, loss: 0.00880120787769556\n",
      "iter 788, loss: 0.010921591892838478\n",
      "iter 789, loss: 0.010705193504691124\n",
      "iter 790, loss: 0.011327490210533142\n",
      "iter 791, loss: 0.010924634523689747\n",
      "iter 792, loss: 0.011355101130902767\n",
      "iter 793, loss: 0.00897500291466713\n",
      "iter 794, loss: 0.010595230385661125\n",
      "iter 795, loss: 0.01129244826734066\n",
      "iter 796, loss: 0.00891010370105505\n",
      "iter 797, loss: 0.012001301161944866\n",
      "iter 798, loss: 0.011758166365325451\n",
      "iter 799, loss: 0.008925038389861584\n",
      "iter 800, loss: 0.01024840958416462\n",
      "iter 801, loss: 0.01157481037080288\n",
      "iter 802, loss: 0.010360879823565483\n",
      "iter 803, loss: 0.008829779922962189\n",
      "iter 804, loss: 0.011401989497244358\n",
      "iter 805, loss: 0.01138509251177311\n",
      "iter 806, loss: 0.010610949248075485\n",
      "iter 807, loss: 0.009352762252092361\n",
      "iter 808, loss: 0.010769663378596306\n",
      "iter 809, loss: 0.012375254184007645\n",
      "iter 810, loss: 0.010660484433174133\n",
      "iter 811, loss: 0.01147423218935728\n",
      "iter 812, loss: 0.011530951596796513\n",
      "iter 813, loss: 0.01038274820894003\n",
      "iter 814, loss: 0.010074120946228504\n",
      "iter 815, loss: 0.009944980032742023\n",
      "iter 816, loss: 0.01171768270432949\n",
      "iter 817, loss: 0.010244674980640411\n",
      "iter 818, loss: 0.0099604781717062\n",
      "iter 819, loss: 0.0114798154681921\n",
      "iter 820, loss: 0.009523389860987663\n",
      "iter 821, loss: 0.010732343420386314\n",
      "iter 822, loss: 0.014210954308509827\n",
      "iter 823, loss: 0.01079856138676405\n",
      "iter 824, loss: 0.009050301276147366\n",
      "iter 825, loss: 0.008574151434004307\n",
      "iter 826, loss: 0.01096688024699688\n",
      "iter 827, loss: 0.011058897711336613\n",
      "iter 828, loss: 0.010077758692204952\n",
      "iter 829, loss: 0.011841755360364914\n",
      "iter 830, loss: 0.010822721756994724\n",
      "iter 831, loss: 0.008736319839954376\n",
      "iter 832, loss: 0.009456807747483253\n",
      "iter 833, loss: 0.01174599677324295\n",
      "iter 834, loss: 0.010721279308199883\n",
      "iter 835, loss: 0.010541926138103008\n",
      "iter 836, loss: 0.009252111427485943\n",
      "iter 837, loss: 0.01077826414257288\n",
      "iter 838, loss: 0.010541689582169056\n",
      "iter 839, loss: 0.009287294931709766\n",
      "iter 840, loss: 0.010142305865883827\n",
      "iter 841, loss: 0.010303282178938389\n",
      "iter 842, loss: 0.010860724374651909\n",
      "iter 843, loss: 0.009559528902173042\n",
      "iter 844, loss: 0.010769722051918507\n",
      "iter 845, loss: 0.010853899642825127\n",
      "iter 846, loss: 0.011247633956372738\n",
      "iter 847, loss: 0.010226311162114143\n",
      "iter 848, loss: 0.010243267752230167\n",
      "iter 849, loss: 0.011747616343200207\n",
      "iter 850, loss: 0.009468386881053448\n",
      "iter 851, loss: 0.00906053651124239\n",
      "iter 852, loss: 0.011450816877186298\n",
      "iter 853, loss: 0.011029710993170738\n",
      "iter 854, loss: 0.011807749047875404\n",
      "iter 855, loss: 0.010976969264447689\n",
      "iter 856, loss: 0.012017790228128433\n",
      "iter 857, loss: 0.009298787452280521\n",
      "iter 858, loss: 0.008881851099431515\n",
      "iter 859, loss: 0.010704591870307922\n",
      "iter 860, loss: 0.01078858319669962\n",
      "iter 861, loss: 0.011028463020920753\n",
      "iter 862, loss: 0.011129804886877537\n",
      "iter 863, loss: 0.01087671983987093\n",
      "iter 864, loss: 0.009346460923552513\n",
      "iter 865, loss: 0.009327681735157967\n",
      "iter 866, loss: 0.009761571884155273\n",
      "iter 867, loss: 0.011863764375448227\n",
      "iter 868, loss: 0.010281465947628021\n",
      "iter 869, loss: 0.012092829681932926\n",
      "iter 870, loss: 0.011332482099533081\n",
      "iter 871, loss: 0.01246787142008543\n",
      "iter 872, loss: 0.013243230059742928\n",
      "iter 873, loss: 0.0095532750710845\n",
      "iter 874, loss: 0.011725523509085178\n",
      "iter 875, loss: 0.00927344523370266\n",
      "iter 876, loss: 0.01007017306983471\n",
      "iter 877, loss: 0.011364109814167023\n",
      "iter 878, loss: 0.010578715242445469\n",
      "iter 879, loss: 0.01149033848196268\n",
      "iter 880, loss: 0.010436289943754673\n",
      "iter 881, loss: 0.010162651538848877\n",
      "iter 882, loss: 0.011042820289731026\n",
      "iter 883, loss: 0.010524935089051723\n",
      "iter 884, loss: 0.011023792438209057\n",
      "iter 885, loss: 0.010092118754982948\n",
      "iter 886, loss: 0.009800193831324577\n",
      "iter 887, loss: 0.0094859404489398\n",
      "iter 888, loss: 0.011843729764223099\n",
      "iter 889, loss: 0.010634797625243664\n",
      "iter 890, loss: 0.009764925576746464\n",
      "iter 891, loss: 0.012528440915048122\n",
      "iter 892, loss: 0.013019049540162086\n",
      "iter 893, loss: 0.010052920319139957\n",
      "iter 894, loss: 0.011657782830297947\n",
      "iter 895, loss: 0.011054884642362595\n",
      "iter 896, loss: 0.010511310771107674\n",
      "iter 897, loss: 0.010280202142894268\n",
      "iter 898, loss: 0.010503455065190792\n",
      "iter 899, loss: 0.009537981823086739\n",
      "iter 900, loss: 0.011933519504964352\n",
      "iter 901, loss: 0.012031343765556812\n",
      "iter 902, loss: 0.009737110696732998\n",
      "iter 903, loss: 0.011130818165838718\n",
      "iter 904, loss: 0.01068140845745802\n",
      "iter 905, loss: 0.009980126284062862\n",
      "iter 906, loss: 0.01093430258333683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 907, loss: 0.011074579320847988\n",
      "iter 908, loss: 0.010780352167785168\n",
      "iter 909, loss: 0.009055730886757374\n",
      "iter 910, loss: 0.011088979430496693\n",
      "iter 911, loss: 0.0102769760414958\n",
      "iter 912, loss: 0.010726856999099255\n",
      "iter 913, loss: 0.010441903956234455\n",
      "iter 914, loss: 0.010847986675798893\n",
      "iter 915, loss: 0.011796323582530022\n",
      "iter 916, loss: 0.01128648966550827\n",
      "iter 917, loss: 0.010189377702772617\n",
      "iter 918, loss: 0.011734587140381336\n",
      "iter 919, loss: 0.010038948617875576\n",
      "iter 920, loss: 0.010363350622355938\n",
      "iter 921, loss: 0.0105472132563591\n",
      "iter 922, loss: 0.009238515049219131\n",
      "iter 923, loss: 0.008667541667819023\n",
      "iter 924, loss: 0.0095879677683115\n",
      "iter 925, loss: 0.010199366137385368\n",
      "iter 926, loss: 0.01177337858825922\n",
      "iter 927, loss: 0.012055839411914349\n",
      "iter 928, loss: 0.010370470583438873\n",
      "iter 929, loss: 0.010325070470571518\n",
      "iter 930, loss: 0.011108648963272572\n",
      "iter 931, loss: 0.008901610039174557\n",
      "iter 932, loss: 0.01036106888204813\n",
      "iter 933, loss: 0.011012990027666092\n",
      "iter 934, loss: 0.010130701586604118\n",
      "iter 935, loss: 0.01059813890606165\n",
      "iter 936, loss: 0.010962330736219883\n",
      "iter 937, loss: 0.009996459819376469\n",
      "iter 938, loss: 0.009916500188410282\n",
      "iter 939, loss: 0.011505463160574436\n",
      "iter 940, loss: 0.009433518163859844\n",
      "iter 941, loss: 0.01013652328401804\n",
      "iter 942, loss: 0.009927021339535713\n",
      "iter 943, loss: 0.010327826254069805\n",
      "iter 944, loss: 0.010934162884950638\n",
      "iter 945, loss: 0.011144531890749931\n",
      "iter 946, loss: 0.01096248347312212\n",
      "iter 947, loss: 0.010856926441192627\n",
      "iter 948, loss: 0.011337685398757458\n",
      "iter 949, loss: 0.009827496483922005\n",
      "iter 950, loss: 0.009414046071469784\n",
      "iter 951, loss: 0.010962757281959057\n",
      "iter 952, loss: 0.009482122957706451\n",
      "iter 953, loss: 0.011653149500489235\n",
      "iter 954, loss: 0.010159475728869438\n",
      "iter 955, loss: 0.010648725554347038\n",
      "iter 956, loss: 0.011907790787518024\n",
      "iter 957, loss: 0.009902744553983212\n",
      "iter 958, loss: 0.010240312665700912\n",
      "iter 959, loss: 0.011243399232625961\n",
      "iter 960, loss: 0.009791035205125809\n",
      "iter 961, loss: 0.010520469397306442\n",
      "iter 962, loss: 0.011463257484138012\n",
      "iter 963, loss: 0.01016922201961279\n",
      "iter 964, loss: 0.010261635296046734\n",
      "iter 965, loss: 0.008948931470513344\n",
      "iter 966, loss: 0.012004386633634567\n",
      "iter 967, loss: 0.00867107417434454\n",
      "iter 968, loss: 0.01154115330427885\n",
      "iter 969, loss: 0.010255856439471245\n",
      "iter 970, loss: 0.009890279732644558\n",
      "iter 971, loss: 0.012320867739617825\n",
      "iter 972, loss: 0.009246869012713432\n",
      "iter 973, loss: 0.010086296126246452\n",
      "iter 974, loss: 0.010746946558356285\n",
      "iter 975, loss: 0.010172678157687187\n",
      "iter 976, loss: 0.0110195092856884\n",
      "iter 977, loss: 0.009595707058906555\n",
      "iter 978, loss: 0.010925296694040298\n",
      "iter 979, loss: 0.01113115344196558\n",
      "iter 980, loss: 0.010212876833975315\n",
      "iter 981, loss: 0.010456368327140808\n",
      "iter 982, loss: 0.009895635768771172\n",
      "iter 983, loss: 0.011169582605361938\n",
      "iter 984, loss: 0.011622002348303795\n",
      "iter 985, loss: 0.010622218251228333\n",
      "iter 986, loss: 0.010508661158382893\n",
      "iter 987, loss: 0.010730339214205742\n",
      "iter 988, loss: 0.010200328193604946\n",
      "iter 989, loss: 0.011061876080930233\n",
      "iter 990, loss: 0.010498838499188423\n",
      "iter 991, loss: 0.010433482006192207\n",
      "iter 992, loss: 0.011485988274216652\n",
      "iter 993, loss: 0.010686619207262993\n",
      "iter 994, loss: 0.009789171628654003\n",
      "iter 995, loss: 0.01196560449898243\n",
      "iter 996, loss: 0.011283540166914463\n",
      "iter 997, loss: 0.008464869111776352\n",
      "iter 998, loss: 0.010668073780834675\n",
      "iter 999, loss: 0.010480819270014763\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "for i,data in enumerate(data_loader):\n",
    "    \n",
    "    inputs,targets = data\n",
    "\n",
    "    inputs = inputs.to(device='cuda')\n",
    "    targets = targets.to(device='cuda')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = cnndnn(inputs)\n",
    "    \n",
    "    #loss = ((output-targets)**2).sum()\n",
    "    loss = ((output-targets)**2).mean()\n",
    "    loss_list.append(loss.cpu().detach().numpy().squeeze())\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('iter {}, loss: {}'.format(i,loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " raw:  (128, 160, 3)\n",
      "torch.Size([10, 128, 160])\n",
      "frame to be predicted:  torch.Size([1, 3, 128, 160])\n",
      "(128, 160)\n"
     ]
    }
   ],
   "source": [
    "# VISUALIZE LAST PREDICTED OUTPUT\n",
    "frame_id = 33\n",
    "\n",
    "# SELECT IMAGE\n",
    "raw = images[frame_id].transpose(1,2,0)\n",
    "print (\" raw: \", raw.shape)\n",
    "\n",
    "ax=plt.subplot(2,2,1)\n",
    "plt.title(\"Input\")\n",
    "plt.imshow(raw)\n",
    "\n",
    "# TARGET\n",
    "ax=plt.subplot(2,2,2)\n",
    "plt.title(\"target\")\n",
    "print (targets.shape)\n",
    "target1 = predictions[frame_id]\n",
    "plt.imshow(target1)\n",
    "\n",
    "\n",
    "# PREDICTION\n",
    "ax=plt.subplot(2,2,3)\n",
    "plt.title(\"prediction\")\n",
    "\n",
    "frame_selected = torch.from_numpy(raw.transpose(2,0,1)).float()[None].to(device='cuda')\n",
    "print (\"frame to be predicted: \", frame_selected.shape)\n",
    "predicted = cnndnn(frame_selected).cpu().detach().numpy().squeeze()\n",
    "print (predicted.shape)\n",
    "plt.imshow(predicted)\n",
    "\n",
    "# LOSS\n",
    "ax=plt.subplot(2,2,4)\n",
    "plt.title(\"loss\")\n",
    "plt.plot(loss_list)\n",
    "plt.xlim(0,len(loss_list))\n",
    "\n",
    "\n",
    "plt.suptitle(\"Predicting frame; \"+str(frame_id))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame selected:  torch.Size([1, 3, 128, 160]) <class 'torch.Tensor'>\n",
      "(128, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "# REPREDICT ON A SELECTED FRAME\n",
    "frame_id = 0\n",
    "\n",
    "#print (images.shape)\n",
    "frame_selected = torch.from_numpy(images[frame_id]).float()[None]\n",
    "print (\"frame selected: \", frame_selected.shape, type(frame_selected[0][0][0][0]))\n",
    "\n",
    "output = cnndnn(frame_selected)\n",
    "\n",
    "\n",
    "raw = inputs[0].cpu().detach().numpy().transpose(1,2,0)\n",
    "print (raw.shape)\n",
    "\n",
    "ax=plt.subplot(2,1,1)\n",
    "plt.imshow(raw)\n",
    "\n",
    "\n",
    "# \n",
    "ax=plt.subplot(2,1,2)\n",
    "predicted = output[0].cpu().detach().numpy().transpose(1,2,0)\n",
    "plt.imshow(predicted)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
